name: Extract Data from WHO PDF

on:
  workflow_run:
    workflows: ["Download Latest WHO PDF"]
    types: [completed]
    branches: [main]
  workflow_dispatch:
    inputs:
      week_number:
        description: 'Week number to extract (leave empty for latest)'
        required: false
        type: string
        default: ''
      year:
        description: 'Year (required if week specified)'
        required: false
        type: string
        default: ''
      model:
        description: 'Model to use'
        required: false
        type: string
        default: 'gpt-5'

jobs:
  extract:
    runs-on: ubuntu-latest
    # Only run if the download workflow succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    outputs:
      week_number: ${{ steps.extract_info.outputs.week_number }}
      year: ${{ steps.extract_info.outputs.year }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true

      - name: Install dependencies
        run: |
          # Install full project dependencies (includes extraction libs)
          uv sync --frozen

      - name: Run extraction
        env:
          DSCI_AZ_OPENAI_API_KEY_WHO_CHOLERA: ${{ secrets.DSCI_AZ_OPENAI_API_KEY_WHO_CHOLERA }}
          DSCI_AZ_BLOB_DEV_SAS: ${{ secrets.DSCI_AZ_BLOB_DEV_SAS_WRITE }}
          DSCI_AZ_BLOB_DEV_SAS_WRITE: ${{ secrets.DSCI_AZ_BLOB_DEV_SAS_WRITE }}
          STAGE: dev
          LOG_BACKEND: duckdb
          PYTHONUNBUFFERED: 1  # Force unbuffered output for real-time logs
        run: |
          # Build arguments
          ARGS=""

          # Week argument
          if [ -n "${{ inputs.week_number }}" ]; then
            ARGS="$ARGS --week ${{ inputs.week_number }}"
          else
            ARGS="$ARGS --week latest"
          fi

          # Year argument (if week specified)
          if [ -n "${{ inputs.year }}" ]; then
            ARGS="$ARGS --year ${{ inputs.year }}"
          fi

          # Model argument
          if [ -n "${{ inputs.model }}" ]; then
            ARGS="$ARGS --model ${{ inputs.model }}"
          fi

          # Run extraction
          uv run python scripts/run_llm_extraction_gha.py $ARGS

      - name: Extract week/year info for post-processing
        id: extract_info
        run: |
          # Determine week/year that was extracted
          if [ -n "${{ inputs.week_number }}" ] && [ -n "${{ inputs.year }}" ]; then
            echo "week_number=${{ inputs.week_number }}" >> $GITHUB_OUTPUT
            echo "year=${{ inputs.year }}" >> $GITHUB_OUTPUT
          else
            # For "latest", we need to determine the current week
            WEEK=$(date +%V)
            YEAR=$(date +%Y)
            echo "week_number=$WEEK" >> $GITHUB_OUTPUT
            echo "year=$YEAR" >> $GITHUB_OUTPUT
          fi

      - name: Create job summary
        if: success()
        run: |
          echo "## Extraction Successful! âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Week**: ${{ inputs.week_number || 'latest' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model**: ${{ inputs.model || 'gpt-5' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Stage**: dev" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Outputs" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“Š Logs uploaded to: \`ds-cholera-pdf-scraper/processed/logs/prompt_logs/\`" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“„ Raw CSV uploaded to: \`ds-cholera-pdf-scraper/raw/monitoring/llm_extractions/\`" >> $GITHUB_STEP_SUMMARY
          echo "- â„¹ï¸  Run post-processing workflow to create processed data" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Query Logs" >> $GITHUB_STEP_SUMMARY
          echo '```python' >> $GITHUB_STEP_SUMMARY
          echo 'from src.cloud_logging import DuckDBCloudQuery' >> $GITHUB_STEP_SUMMARY
          echo 'query = DuckDBCloudQuery()' >> $GITHUB_STEP_SUMMARY
          echo 'df = query.get_latest_runs(n=5)' >> $GITHUB_STEP_SUMMARY
          echo 'print(df)' >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Handle failure
        if: failure()
        run: |
          echo "## Extraction Failed âŒ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check the logs above for error details." >> $GITHUB_STEP_SUMMARY

  post-process:
    needs: extract
    if: success()
    uses: ./.github/workflows/post-process-extractions.yml
    with:
      source: llm
      week_number: ${{ needs.extract.outputs.week_number }}
      year: ${{ needs.extract.outputs.year }}
      correct_gap_fill: false
    secrets:
      DSCI_AZ_BLOB_DEV_SAS_WRITE: ${{ secrets.DSCI_AZ_BLOB_DEV_SAS_WRITE }}
