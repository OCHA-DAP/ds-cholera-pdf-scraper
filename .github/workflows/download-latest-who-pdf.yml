name: Download Latest WHO PDF

on:
  workflow_dispatch:
    inputs:
      week_number:
        description: 'Week number to download (leave empty for latest)'
        required: false
        type: string
        default: ''
      upload_to_blob:
        description: 'Upload to Azure blob storage'
        required: false
        type: boolean
        default: false
  schedule:
    - cron: '47 13 * * 2,5'

jobs:
  download-pdf:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv and add to PATH
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          # Ensure uv is present and executable; report version for debugging
          if [ -x "$HOME/.cargo/bin/uv" ]; then
            "$HOME/.cargo/bin/uv" --version || true
          else
            echo "ERROR: uv not found at $HOME/.cargo/bin/uv" >&2
            ls -la "$HOME/.cargo/bin" || true
            exit 1
          fi

      - name: Install system dependencies
        run: |
          sudo apt-get update
          # Install Chrome instead of Chromium for better Selenium compatibility
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install Python dependencies
        run: |
          # Ensure the uv binary directory is available in this step's PATH
          export PATH="$HOME/.cargo/bin:$PATH"
          if [ -x "$HOME/.cargo/bin/uv" ]; then
            "$HOME/.cargo/bin/uv" sync --frozen
          else
            echo "ERROR: uv not available; cannot install dependencies via uv" >&2
            exit 1
          fi

      - name: Create output directory
        run: mkdir -p downloads

      - name: Download WHO PDF
        env:
          STAGE: dev
          DSCI_AZ_BLOB_DEV_SAS_WRITE: ${{ secrets.DSCI_AZ_BLOB_DEV_SAS_WRITE }}
        run: |
          # Always upload to blob for scheduled runs; manual runs can opt-in
          ARGS="--output-dir downloads --save-metadata downloads/metadata.json"

          if [ -n "${{ inputs.week_number }}" ]; then
            ARGS="$ARGS --week ${{ inputs.week_number }}"
            echo "Downloading week ${{ inputs.week_number }}..."
          else
            echo "Downloading latest week..."
            # For scheduled runs (no manual input), always upload
            if [ "${{ github.event_name }}" = "schedule" ]; then
              ARGS="$ARGS --upload"
            fi
          fi

          # Manual runs: respect the upload toggle
          if [ "${{ inputs.upload_to_blob }}" = "true" ]; then
            ARGS="$ARGS --upload"
          fi

          python scripts/download_latest_who_pdf.py $ARGS

      - name: Display summary
        if: always()
        run: |
          echo "=== Download Summary ==="
          ls -lh downloads/*.pdf 2>/dev/null || echo "No PDFs found"

          if [ -f "downloads/metadata.json" ]; then
            echo ""
            echo "=== Bulletin Metadata ==="
            cat downloads/metadata.json | python -m json.tool
          fi

      - name: Upload artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: who-cholera-pdf-${{ github.run_number }}
          path: downloads/
          retention-days: 90

      - name: Create job summary
        if: success()
        run: |
          echo "## Download Successful! ✅" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "downloads/metadata.json" ]; then
            WEEK=$(python -c "import json; print(json.load(open('downloads/metadata.json'))['week'])")
            YEAR=$(python -c "import json; print(json.load(open('downloads/metadata.json'))['year'])")
            DATE_RANGE=$(python -c "import json; print(json.load(open('downloads/metadata.json'))['date_range'])")

            echo "### Bulletin Details" >> $GITHUB_STEP_SUMMARY
            echo "- **Week**: $WEEK" >> $GITHUB_STEP_SUMMARY
            echo "- **Year**: $YEAR" >> $GITHUB_STEP_SUMMARY
            echo "- **Date Range**: $DATE_RANGE" >> $GITHUB_STEP_SUMMARY
          fi

          PDF=$(ls downloads/*.pdf 2>/dev/null | head -1)
          if [ -f "$PDF" ]; then
            SIZE=$(du -h "$PDF" | cut -f1)
            NAME=$(basename "$PDF")
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Downloaded File" >> $GITHUB_STEP_SUMMARY
            echo "- **Filename**: \`$NAME\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Size**: $SIZE" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ inputs.upload_to_blob }}" = "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Blob Storage" >> $GITHUB_STEP_SUMMARY
            echo "☁️ PDF uploaded to Azure blob storage" >> $GITHUB_STEP_SUMMARY
          fi
